<!doctype html>
<html>
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <title>LLM Chat with Transformers.js.py</title>
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/@stlite/browser@0.76.0/build/style.css"
    />
  </head>
  <body>
    <div id="root"></div>
    <script type="module">
      import { mount } from "https://cdn.jsdelivr.net/npm/@stlite/browser@0.76.0/build/stlite.js";
      mount(
        `
import streamlit as st

# Install transformers_js_py
from micropip import install
await install('transformers_js_py')

# Import the package
from transformers_js_py import import_transformers_js

# Page config
st.set_page_config(
    page_title="LLM Chat",
    page_icon="ðŸ¤–",
    layout="wide"
)

# Model options
MODEL_OPTIONS = {
    "TinyLlama Chat 1.1B": "Xenova/TinyLlama-1.1B-Chat-v1.0",
    "DistilGPT2": "distilgpt2",
    "GPT2": "gpt2",
    "Gemma 2B": "Xenova/gemma-2b-it"
}

# Initialize session state
if "model_loaded" not in st.session_state:
    st.session_state.model_loaded = False
if "conversations" not in st.session_state:
    st.session_state.conversations = []
if "current_conversation" not in st.session_state:
    st.session_state.current_conversation = []
if "selected_model" not in st.session_state:
    st.session_state.selected_model = "TinyLlama Chat 1.1B"
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = "You are a helpful and friendly AI assistant."

# Sidebar for model management
with st.sidebar:
    st.title("ðŸ¤– Model Controls")
    
    # Model selection dropdown
    selected_model = st.selectbox(
        "Select Model",
        list(MODEL_OPTIONS.keys()),
        index=list(MODEL_OPTIONS.keys()).index(st.session_state.selected_model)
    )
    
    # Update selected model in session state
    if selected_model != st.session_state.selected_model:
        st.session_state.selected_model = selected_model
        st.session_state.model_loaded = False
    
    # System prompt for chat
    system_prompt = st.text_area(
        "System Prompt",
        st.session_state.system_prompt,
        help="Set the personality or behavior of the assistant"
    )
    
    if system_prompt != st.session_state.system_prompt:
        st.session_state.system_prompt = system_prompt
    
    # Model loading/unloading buttons
    if not st.session_state.model_loaded:
        if st.button("Load Model", type="primary"):
            with st.spinner(f"Loading {st.session_state.selected_model}..."):
                try:
                    transformers = await import_transformers_js()
                    pipeline = transformers.pipeline
                    model_id = MODEL_OPTIONS[st.session_state.selected_model]
                    st.session_state["pipe"] = await pipeline('text-generation', model_id)
                    st.session_state.model_loaded = True
                    st.success(f"{st.session_state.selected_model} loaded successfully!")
                except Exception as e:
                    st.error(f"Error loading model: {str(e)}")
    else:
        if st.button("Unload Model", type="secondary"):
            st.session_state.model_loaded = False
            st.session_state["pipe"] = None
            st.success("Model unloaded!")
    
    st.markdown("---")
    st.markdown("### Conversations")
    
    # Display conversation list
    for i, conv in enumerate(st.session_state.conversations):
        if st.button(f"Conversation {i+1}", key=f"conv_{i}"):
            st.session_state.current_conversation = conv
    
    # New conversation button
    if st.button("New Conversation"):
        if len(st.session_state.current_conversation) > 0:
            st.session_state.conversations.append(st.session_state.current_conversation)
        st.session_state.current_conversation = []
        st.rerun()

# Main chat interface
st.title("ðŸ’¬ LLM Chat")

# Display current conversation
for message in st.session_state.current_conversation:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# Chat input
if prompt := st.chat_input("What would you like to say?"):
    if not st.session_state.model_loaded:
        st.error("Please load the model first!")
    else:
        # Add user message to conversation
        st.session_state.current_conversation.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.write(prompt)
        
        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                pipe = st.session_state["pipe"]
                
                try:
                    # Format messages for LLM chat models
                    if "TinyLlama" in st.session_state.selected_model or "Gemma" in st.session_state.selected_model:
                        # Format messages for chat models
                        messages = [
                            {"role": "system", "content": st.session_state.system_prompt}
                        ]
                        
                        # Add conversation history
                        for msg in st.session_state.current_conversation:
                            messages.append({
                                "role": msg["role"],
                                "content": msg["content"]
                            })
                        
                        response = await pipe(messages, {"max_new_tokens": 500})
                        assistant_response = response[0]["generated_text"][-1]["content"]
                    else:
                        # For regular text generation models, just use the prompt
                        response = await pipe(prompt, {"max_length": 200})
                        assistant_response = response[0]["generated_text"]
                    
                    # Display the response
                    st.write(assistant_response)
                    
                    # Add assistant response to conversation
                    st.session_state.current_conversation.append({
                        "role": "assistant",
                        "content": assistant_response
                    })
                except Exception as e:
                    st.error(f"Error generating response: {str(e)}")

# Model status
st.sidebar.markdown("---")
st.sidebar.markdown("### Model Status")
if st.session_state.model_loaded:
    st.sidebar.success(f"Model: {st.session_state.selected_model}")
    model_id = MODEL_OPTIONS[st.session_state.selected_model]
    st.sidebar.info(f"Model ID: {model_id}")
else:
    st.sidebar.warning("No model loaded")
`,
        document.getElementById("root"),
      );
    </script>
  </body>
</html>